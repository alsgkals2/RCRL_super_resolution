{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import modules.dist_model\n",
    "from utils import util\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "from modules import * \n",
    "import modules\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from util.utils import calculate_psnr, _ssim, Logger, RandCrop, RandHorizontalFlip, RandRotate, ToTensor, VGG19PerceptualLoss\n",
    "from util.utils_common import calculate_psnr as PSNR\n",
    "from util.utils_common import calculate_ssim as SSIM\n",
    "from util.utils_common import calc_metrics as CALC\n",
    "from memory.storage import Storage\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opt.option_hrlr_cycle_scheduler import args\n",
    "from data.LQGT_dataset_hrlr_BICUBIC_scale4 import LQGTDataset, Testdataset\n",
    "from model import encoder_x4 as encoder\n",
    "\n",
    "from model import decoder_scale4 as decoder\n",
    "from model import discriminator_scale4 as discriminator\n",
    "\n",
    "args.ratio_x = 'x4'\n",
    "args.scale = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opt.option_hrlr_cycle import args\n",
    "from data.LQGT_dataset_hrlr_BICUBIC_scale2 import LQGTDataset, Testdataset\n",
    "from model import encoder_x2 as encoder\n",
    "\n",
    "from model import decoder_scale2 as decoder\n",
    "from model import discriminator_scale2 as discriminator\n",
    "\n",
    "args.ratio_x = 'x2'\n",
    "args.scale = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyper-parameter and Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.gpu_id='0'\n",
    "args.lr_G = 5e-5\n",
    "args.lr_D = 5e-5\n",
    "device = 'cuda'\n",
    "args.cycle_mode=True\n",
    "args.shuffle_mode=True\n",
    "args.n_gen=2\n",
    "args.lambda_align = 0.01\n",
    "args.eval_mode = True\n",
    "args.sub_file_name = f'TEMP'\n",
    "\n",
    "data_name_test = ['Deepfake','Face2Face','FaceSwap','NeuralTextures','DFDC_trans'] ### please input the name you will evaluate\n",
    "\n",
    "#set_seed\n",
    "seed = 2020\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # for faster training, but not deterministic\n",
    "# device setting\n",
    "if args.gpu_id is not None:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "else:\n",
    "    print('use --gpu_id to specify GPU ID to use')\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Enc = encoder.Encoder_RRDB(num_feat = args.n_hidden_feats).cuda()\n",
    "model_Dec_Id = decoder.Decoder_Id_RRDB(num_in_ch=args.n_hidden_feats).cuda() #Gt\n",
    "model_Dec_SR = decoder.Decoder_SR_RRDB(num_in_ch=args.n_hidden_feats, num_block=args.n_sr_block).cuda() #Gsr\n",
    "model_Disc_feat = discriminator.DiscriminatorVGG(in_ch=args.n_hidden_feats, image_size=args.patch_size).cuda()\n",
    "model_Disc_img_LR = discriminator.DiscriminatorVGG(in_ch=3, image_size=args.patch_size).cuda()\n",
    "model_Disc_img_HR = discriminator.DiscriminatorVGG(in_ch=3, image_size=args.patch_size*args.scale).cuda()\n",
    "\n",
    "model_Enc = nn.DataParallel(model_Enc).eval().to(device)\n",
    "model_Dec_Id= nn.DataParallel(model_Dec_Id).eval().to(device)\n",
    "model_Dec_SR= nn.DataParallel(model_Dec_SR).eval().to(device)\n",
    "model_Disc_feat= nn.DataParallel(model_Disc_feat).eval().to(device)\n",
    "model_Disc_img_LR= nn.DataParallel(model_Disc_img_LR).eval().to(device)\n",
    "model_Disc_img_HR= nn.DataParallel(model_Disc_img_HR).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_L1 = nn.L1Loss().cuda()\n",
    "loss_MSE = nn.MSELoss().cuda()\n",
    "loss_adversarial = nn.BCEWithLogitsLoss().cuda()\n",
    "loss_percept = VGG19PerceptualLoss().cuda()\n",
    "\n",
    "\n",
    "# optimizer \n",
    "params_G = list(model_Enc.parameters()) + list(model_Dec_Id.parameters()) + list(model_Dec_SR.parameters())\n",
    "optimizer_G = optim.Adam(\n",
    "    params_G,\n",
    "    lr=args.lr_G,\n",
    "    betas=(args.beta1, args.beta2),\n",
    "    weight_decay=args.weight_decay,\n",
    "    amsgrad=True\n",
    ")\n",
    "params_D = list(model_Disc_feat.parameters()) + list(model_Disc_img_LR.parameters()) + list(model_Disc_img_HR.parameters())\n",
    "optimizer_D = optim.Adam(\n",
    "    params_D,\n",
    "    lr=args.lr_D,\n",
    "    betas=(args.beta1, args.beta2),\n",
    "    weight_decay=args.weight_decay,\n",
    "    amsgrad=True\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "iter_indices = [args.interval1, args.interval2, args.interval3]\n",
    "scheduler_G = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer=optimizer_G,\n",
    "    milestones=iter_indices,\n",
    "    gamma=0.5\n",
    ")\n",
    "scheduler_D = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer=optimizer_D,\n",
    "    milestones=iter_indices,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "args.checkpoint = './task1_storage.pth' ### please input the path of weight you will evaluate\n",
    "\n",
    "args.snap_path = os.path.dirname(args.checkpoint).replace('weights','results')\n",
    "# load model weights & optimzer % scheduler\n",
    "if args.checkpoint is not None:\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "\n",
    "    model_Enc.load_state_dict(checkpoint['model_Enc'])\n",
    "    model_Dec_Id.load_state_dict(checkpoint['model_Dec_Id'])\n",
    "    model_Dec_SR.load_state_dict(checkpoint['model_Dec_SR'])\n",
    "    model_Disc_feat.load_state_dict(checkpoint['model_Disc_feat'])\n",
    "    model_Disc_img_LR.load_state_dict(checkpoint['model_Disc_img_LR'])\n",
    "    model_Disc_img_HR.load_state_dict(checkpoint['model_Disc_img_HR'])\n",
    "\n",
    "    optimizer_D.load_state_dict(checkpoint['optimizer_D'])\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G'])\n",
    "\n",
    "    scheduler_D.load_state_dict(checkpoint['scheduler_D'])\n",
    "    scheduler_G.load_state_dict(checkpoint['scheduler_G'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_log(path_folder):\n",
    "    os.makedirs(path_folder, exist_ok=True)\n",
    "    cnt=0\n",
    "    name_file = f'eval_result_v{cnt}'\n",
    "    while os.path.exists(os.path.join(path_folder,f'{name_file}.txt')):\n",
    "        cnt += 1\n",
    "        name_file = f'eval_result_v{cnt}'\n",
    "    print(f'name_file : {name_file}')\n",
    "    log = Logger()\n",
    "    log.open(os.path.join(path_folder,f'{name_file}.txt'))\n",
    "    log.write('\\n')\n",
    "    log.write(f'checkpoint path: {args.checkpoint} \\n')\n",
    "    return log\n",
    "\n",
    "class PerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, model='net-lin', net='vgg', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0]): # VGG using our perceptually-learned weights (LPIPS metric)\n",
    "    # def __init__(self, model='net', net='vgg', use_gpu=True): # \"default\" way of using VGG as a perceptual loss\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        print('Setting up Perceptual loss...')\n",
    "        print(f\" LPIPS net is {net}\")\n",
    "        self.use_gpu = use_gpu\n",
    "        self.spatial = spatial\n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.model = dist_model.DistModel()\n",
    "        self.model.initialize(model=model, net=net, use_gpu=use_gpu, colorspace=colorspace, spatial=self.spatial, gpu_ids=gpu_ids)\n",
    "        print('...[%s] initialized'%self.model.name())\n",
    "        print('...Done')\n",
    "\n",
    "    def forward(self, pred, target, normalize=False):\n",
    "        \"\"\"\n",
    "        Pred and target are Variables.\n",
    "        If normalize is True, assumes the images are between [0,1] and then scales them between [-1,+1]\n",
    "        If normalize is False, assumes the images are already between [-1,+1]\n",
    "\n",
    "        Inputs pred and target are Nx3xHxW\n",
    "        Output pytorch Variable N long\n",
    "        \"\"\"\n",
    "\n",
    "        if normalize:\n",
    "            target = 2 * target  - 1\n",
    "            pred = 2 * pred  - 1\n",
    "\n",
    "        return self.model.forward(target, pred)\n",
    "    \n",
    "lpips = modules.PerceptualLoss(model='net-lin',net='vgg',use_gpu=True).eval() # alex, squeeze, vgg\n",
    "lpips_alex = modules.PerceptualLoss(model='net-lin',net='alex',use_gpu=True).eval() # alex, squeeze, vgg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_path_test = []\n",
    "path_imgs = args.checkpoint[:-4].replace('weights','results')\n",
    "log = make_eval_log(path_imgs)\n",
    "\n",
    "for _data_name in data_name_test:\n",
    "    full_path = os.path.join(path_imgs,_data_name)\n",
    "    _path_test = f'/media/data1/DS/{args.ratio_x}/{_data_name}/test' # Change as needed\n",
    "    list_path_test.append((_path_test,full_path))\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_test_loader=[]\n",
    "for _path, _save_path in list_path_test:\n",
    "    test_dataset = Testdataset(\n",
    "        os.path.join(_path,'HR'),\n",
    "        os.path.join(_path,'LR'),\n",
    "        transform=transforms.Compose([ToTensor()]),\n",
    "        patch_size = args.patch_size,\n",
    "        scale=2,\n",
    "        crop_mode = False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        copy.deepcopy(test_dataset),\n",
    "        batch_size=1,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=False,\n",
    "        shuffle=False\n",
    "    )\n",
    "    list_test_loader.append(copy.deepcopy(test_loader))\n",
    "    print(f'path is : {_path}')\n",
    "    print(f'len of dataset is : {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_v2(loader,\n",
    "         save_path,\n",
    "         log,\n",
    "         models,\n",
    "         curr_epoch = 0,\n",
    "         show_mode = False,\n",
    "         n_interval = 100,\n",
    "         save_mode = False):\n",
    "    model_Enc.eval()\n",
    "    model_Dec_SR.eval()\n",
    "    list_psnr, list_ssim = [],[]\n",
    "    list_psnr_origin, list_ssim_origin, list_psnr_v2, list_ssim_v2 = [],[],[],[]\n",
    "    list_lpips_origin, list_lpips_v2 = [],[]\n",
    "    list_lpips_a_origin, list_lpips_a_v2 = [],[]\n",
    "    \n",
    "    list_lqhq_psnr=[]\n",
    "    n_scale = 4 if args.ratio_x == 'x4' else 2\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (data, filename) in enumerate(loader):\n",
    "            X_t = data['img_LQ'].cuda(non_blocking=True)\n",
    "            _X_origin = copy.deepcopy(X_t)\n",
    "            Y = data['img_GT'].cuda(non_blocking=True)#.squeeze()\n",
    "            # real label and fake label\n",
    "            batch_size = X_t.size(0)\n",
    "            # inference output\n",
    "            feat = models[0](X_t) #encoder\n",
    "            out = models[1](feat) #decoder(generator)\n",
    "            if not (Y.shape== out.shape):\n",
    "                out = F.interpolate(out,size=(Y.shape[2],Y.shape[3]))\n",
    "            min_max = (0, 1)\n",
    "            out = out.detach().float().cpu().clamp_(*min_max)\n",
    "            _X_origin = F.interpolate(_X_origin,size=(Y.shape[2],Y.shape[3]))\n",
    "\n",
    "            X_t = F.interpolate(X_t.detach().cpu(),scale_factor = n_scale)\n",
    "            if show_mode or save_mode :\n",
    "                img_lq = transforms.ToPILImage()(X_t.squeeze())\n",
    "                img_output = transforms.ToPILImage()(out.squeeze())\n",
    "                img_y = transforms.ToPILImage()(Y.squeeze())\n",
    "                if show_mode and idx % n_interval == 0:\n",
    "                    img_lq.show(); img_output.show(); img_y.show()\n",
    "                if save_mode:\n",
    "                    img_lq.save(f'{save_path}/LQ_e{curr_epoch}_{idx}.png')\n",
    "                    img_output.save(f'{save_path}/OUTPUT_e{curr_epoch}_{idx}.png')\n",
    "                    img_y.save(f'{save_path}/HQ_e{curr_epoch}_{idx}.png')\n",
    "                    \n",
    "            __X_origin = np.array(_X_origin.squeeze().permute(1,2,0).detach().cpu()*255.0).round()\n",
    "            _out = np.array(out.squeeze().permute(1,2,0).detach().cpu()*255.0).round()#.squeeze()\n",
    "            _Y = np.array(Y.squeeze().permute(1,2,0).detach().cpu()*255.0).round()#.squeeze()\n",
    "            _out = (_out - min_max[0]) / (min_max[1] - min_max[0])\n",
    "            \n",
    "            psnr_v2 = PSNR(_out, _Y, border=0)\n",
    "            psnr_v2_origin = PSNR(__X_origin ,_Y, border=0)\n",
    "            ssim_v2 = SSIM(_out, _Y, border=0)\n",
    "            ssim_v2_origin = SSIM(__X_origin ,_Y, border=0)\n",
    "            list_psnr_v2.append(psnr_v2)\n",
    "            list_psnr_origin.append(psnr_v2_origin)\n",
    "            list_ssim_v2.append(ssim_v2)\n",
    "            list_ssim_origin.append(ssim_v2_origin)\n",
    "            \n",
    "            #lpips\n",
    "            img_x = modules.normalize_tensor(Y)\n",
    "            pred_xhat = modules.normalize_tensor(_X_origin)\n",
    "            lpips_dist_origin = lpips.forward(img_x,pred_xhat).item()\n",
    "            lpips_dist_a_origin = lpips_alex.forward(img_x,pred_xhat).item()\n",
    "            list_lpips_origin.append(lpips_dist_origin)\n",
    "            list_lpips_a_origin.append(lpips_dist_a_origin)\n",
    "            \n",
    "            pred_xhat = modules.normalize_tensor(out)\n",
    "            lpips_dist_v2 = lpips.forward(img_x,pred_xhat).item()\n",
    "            lpips_dist_a_v2 = lpips_alex.forward(img_x,pred_xhat).item()\n",
    "            list_lpips_v2.append(lpips_dist_v2)\n",
    "            list_lpips_a_v2.append(lpips_dist_a_v2)\n",
    "            \n",
    "    final_psnr_v2_origin, final_psnr_v2, final_ssim_v2, final_ssim_v2_origin, final_lpips_v2_origin, final_lpips_v2 = 0,0,0,0,0,0\n",
    "    \n",
    "    final_psnr_v2_origin = sum(list_psnr_origin)/len(list_psnr_origin)\n",
    "    final_psnr_v2 = sum(list_psnr_v2)/len(list_psnr_v2)\n",
    "    final_ssim_v2_origin = sum(list_ssim_origin)/len(list_ssim_origin)\n",
    "    final_ssim_v2 = sum(list_ssim_v2)/len(list_ssim_v2)\n",
    "    final_lpips_v2_origin = sum(list_lpips_origin)/len(list_lpips_origin)\n",
    "    final_lpips_v2 = sum(list_lpips_v2)/len(list_lpips_v2)\n",
    "    final_lpips_a_origin = sum(list_lpips_a_origin)/len(list_lpips_a_origin)\n",
    "    final_lpips_a = sum(list_lpips_a_v2)/len(list_lpips_a_v2)\n",
    "    log.write(f'===> psnr_origin: {final_psnr_v2_origin}, ssim_origin: {final_ssim_v2_origin}, lpips_origin: {final_lpips_v2_origin}, lpips_alex: {final_lpips_a_origin} \\n')\n",
    "    log.write(f'===> psnr: {final_psnr_v2}, ssim_v2: {final_ssim_v2}, lpips_v2: {final_lpips_v2}, lpips_alex: {final_lpips_a} \\n')\n",
    "\n",
    "\n",
    "best_psnr = 0\n",
    "if not args.eval_mode : args.epochs = start_epoch+1\n",
    "print('path img : ', path_imgs)\n",
    "iter = 0\n",
    "for idx, (_loader, _save_path) in enumerate(zip(list_test_loader, list_path_test)):\n",
    "    log.write(f'{_save_path[0]} \\n')\n",
    "    Test_v2(_loader, _save_path[1], log, [model_Enc, model_Dec_SR], curr_epoch = start_epoch, show_mode=False, n_interval=20, save_mode = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
