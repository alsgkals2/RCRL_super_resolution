{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from opt.option_hrlr_cycle_scheduler import args\n",
    "from data.LQGT_dataset_hrlr_BICUBIC_scale2 import LQGTDataset, Testdataset\n",
    "from util.utils import calculate_psnr, _ssim, Logger, RandCrop, RandHorizontalFlip, RandRotate, ToTensor, VGG19PerceptualLoss\n",
    "from util.utils_common import calculate_psnr as PSNR\n",
    "from util.utils_common import calculate_ssim as SSIM\n",
    "from util.utils_common import calc_metrics as CALC\n",
    "\n",
    "from model import encoder_x2 as encoder\n",
    "from model import decoder_scale2 as decoder\n",
    "from model import discriminator_scale2 as discriminator\n",
    "from memory.storage_s_feature15_rede2 import Storage\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file = 'Input the name with path'\n",
    "snap_path = 'input the path for saving weight file'\n",
    "checkpoint = 'weight file For Transfer Learning'\n",
    "args.gpu_id = '1,2,3'\n",
    "device='cuda'\n",
    "\n",
    "args.lr_G = 5e-5\n",
    "args.lr_D = 5e-5\n",
    "args.cycle_mode=True\n",
    "args.shuffle_mode=True\n",
    "args.n_gen=2\n",
    "args.scale = 2\n",
    "args.lambda_align = 0.01\n",
    "args.epochs = 200\n",
    "args.sub_file_name = f'hrlr_lrisorigin_encodreX{args.scale}_alignlambdais{args.lambda_align}_storage_softmax_bicubic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging & Set Seed & Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = Logger()\n",
    "data_name = 'DIV2K'\n",
    "print(f'data name is {data_name}')\n",
    "\n",
    "#Logging\n",
    "out_dir = f'./results/{name_file}_{args.sub_file_name}'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "log.open(os.path.join(out_dir,'[log_train]'+f'{name_file}_{args.sub_file_name}.txt'))\n",
    "log.write('\\n')\n",
    "\n",
    "#set_seed\n",
    "seed = 2020\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # for faster training, but not deterministic\n",
    "# device setting\n",
    "if args.gpu_id is not None:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "    log.write('using GPU %s' % args.gpu_id)\n",
    "else:\n",
    "    print('use --gpu_id to specify GPU ID to use')\n",
    "    exit()\n",
    "\n",
    "\n",
    "# make directory for saving weights\n",
    "if not os.path.exists(snap_path):\n",
    "    os.mkdir(snap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "train_dataset = LQGTDataset(\n",
    "    args.dir_lr, args.dir_gt,\n",
    "    transform=transforms.Compose([ToTensor()]),\n",
    "    patch_size = args.patch_size,\n",
    "    scale=2,\n",
    "    shuffle_mode = args.shuffle_mode\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "# load val dataset\n",
    "test_dataset = Testdataset(\n",
    "    args.dir_gt,args.dir_lr,\n",
    "    transform=transforms.Compose([ToTensor()]),\n",
    "    patch_size = args.patch_size,\n",
    "    scale=2,\n",
    "    crop_mode = False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=False,\n",
    "    shuffle=False\n",
    ")\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model (generator)\n",
    "model_Enc = encoder.Encoder_RRDB(num_feat = args.n_hidden_feats).cuda()\n",
    "model_Dec_Id = decoder.Decoder_Id_RRDB(num_in_ch=args.n_hidden_feats).cuda() #Gt\n",
    "model_Dec_SR = decoder.Decoder_SR_RRDB(num_in_ch=args.n_hidden_feats, num_block=args.n_sr_block).cuda() #Gsr\n",
    "model_Disc_feat = discriminator.DiscriminatorVGG(in_ch=args.n_hidden_feats, image_size=args.patch_size).cuda()\n",
    "model_Disc_img_LR = discriminator.DiscriminatorVGG(in_ch=3, image_size=args.patch_size).cuda()\n",
    "model_Disc_img_HR = discriminator.DiscriminatorVGG(in_ch=3, image_size=args.patch_size*args.scale).cuda()\n",
    "\n",
    "model_Enc = nn.DataParallel(model_Enc).eval().to(device)\n",
    "model_Dec_Id= nn.DataParallel(model_Dec_Id).eval().to(device)\n",
    "model_Dec_SR= nn.DataParallel(model_Dec_SR).eval().to(device)\n",
    "model_Disc_feat= nn.DataParallel(model_Disc_feat).eval().to(device)\n",
    "model_Disc_img_LR= nn.DataParallel(model_Disc_img_LR).eval().to(device)\n",
    "model_Disc_img_HR= nn.DataParallel(model_Disc_img_HR).eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration or Load Pre-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_L1 = nn.L1Loss().cuda()\n",
    "loss_MSE = nn.MSELoss().cuda()\n",
    "loss_adversarial = nn.BCEWithLogitsLoss().cuda()\n",
    "loss_percept = VGG19PerceptualLoss().cuda()\n",
    "\n",
    "\n",
    "# optimizer \n",
    "params_G = list(model_Enc.parameters()) + list(model_Dec_Id.parameters()) + list(model_Dec_SR.parameters())\n",
    "optimizer_G = optim.Adam(\n",
    "    params_G,\n",
    "    lr=args.lr_G,\n",
    "    betas=(args.beta1, args.beta2),\n",
    "    weight_decay=args.weight_decay,\n",
    "    amsgrad=True\n",
    ")\n",
    "params_D = list(model_Disc_feat.parameters()) + list(model_Disc_img_LR.parameters()) + list(model_Disc_img_HR.parameters())\n",
    "optimizer_D = optim.Adam(\n",
    "    params_D,\n",
    "    lr=args.lr_D,\n",
    "    betas=(args.beta1, args.beta2),\n",
    "    weight_decay=args.weight_decay,\n",
    "    amsgrad=True\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "iter_indices = [args.interval1, args.interval2, args.interval3]\n",
    "scheduler_G = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer=optimizer_G,\n",
    "    milestones=iter_indices,\n",
    "    gamma=0.5\n",
    ")\n",
    "scheduler_D = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer=optimizer_D,\n",
    "    milestones=iter_indices,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "args.checkpoint = checkpoint\n",
    "# load model weights & optimzer % scheduler\n",
    "if args.checkpoint is not None:\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "\n",
    "    model_Enc.load_state_dict(checkpoint['model_Enc'])\n",
    "    model_Dec_Id.load_state_dict(checkpoint['model_Dec_Id'])\n",
    "    model_Dec_SR.load_state_dict(checkpoint['model_Dec_SR'])\n",
    "    model_Disc_feat.load_state_dict(checkpoint['model_Disc_feat'])\n",
    "    model_Disc_img_LR.load_state_dict(checkpoint['model_Disc_img_LR'])\n",
    "    model_Disc_img_HR.load_state_dict(checkpoint['model_Disc_img_HR'])\n",
    "\n",
    "    optimizer_D.load_state_dict(checkpoint['optimizer_D'])\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G'])\n",
    "\n",
    "    scheduler_D.load_state_dict(checkpoint['scheduler_D'])\n",
    "    scheduler_G.load_state_dict(checkpoint['scheduler_G'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    _Storage_disc = checkpoint['storage_disc']\n",
    "    _Storage_gene = checkpoint['storage_gene']\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    _Storage_disc = Storage(2, 'euclid')\n",
    "    _Storage_gene = Storage(2, 'euclid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check arguments of logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "log.write(' '.join(f'{k}={v} \\n' for k, v in vars(args).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Test_v2(loader,\n",
    "         save_path,\n",
    "         log,\n",
    "         models,\n",
    "         curr_epoch = 0,\n",
    "         show_mode = False,\n",
    "         n_interval = 100,\n",
    "         save_mode = False):\n",
    "    model_Enc.eval()\n",
    "    model_Dec_SR.eval()\n",
    "    list_psnr, list_ssim = [],[]\n",
    "    list_psnr_origin, list_ssim_origin, list_psnr_v2, list_ssim_v2 = [],[],[],[]\n",
    "    list_lqhq_psnr=[]\n",
    "    print(f'save path : {save_path}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (data, filename) in enumerate(loader):\n",
    "            X_t = data['img_LQ'].cuda(non_blocking=True)\n",
    "            _X_origin = copy.deepcopy(X_t)\n",
    "            Y = data['img_GT'].cuda(non_blocking=True)#.squeeze()\n",
    "            # real label and fake label\n",
    "            batch_size = X_t.size(0)\n",
    "            # inference output\n",
    "            feat = models[0](X_t) #encoder\n",
    "            out = models[1](feat) #decoder(generator)\n",
    "\n",
    "            assert(Y.shape== out.shape)\n",
    "            min_max = (0, 1)\n",
    "            out = out.detach().float().cpu().clamp_(*min_max)\n",
    "            _X_origin = F.interpolate(_X_origin, scale_factor=(4,4))\n",
    "            out, Y = out.squeeze(), Y.squeeze()\n",
    "            if (show_mode or save_mode) and idx % n_interval == 0 :\n",
    "                img_lq = transforms.ToPILImage()(X_t.squeeze())\n",
    "                img_output = transforms.ToPILImage()(out)\n",
    "                img_y = transforms.ToPILImage()(Y)\n",
    "                if show_mode:\n",
    "                    img_lq.show(); img_output.show(); img_y.show()\n",
    "                if save_mode:\n",
    "                    img_lq.save(f'{save_path}/LQ_e{curr_epoch}_{idx}.png')\n",
    "                    img_output.save(f'{save_path}/OUTPUT_e{curr_epoch}_{idx}.png')\n",
    "                    img_y.save(f'{save_path}/HQ_e{curr_epoch}_{idx}.png')\n",
    "            _X_origin = (_X_origin.squeeze()*255.0).round().squeeze()\n",
    "            out = (out*255.0).round().squeeze()\n",
    "            Y = (Y*255.0).round().squeeze()\n",
    "            out = out.permute(1,2,0)\n",
    "            Y = Y.permute(1,2,0)\n",
    "            _X_origin = _X_origin.permute(1,2,0)\n",
    "            \n",
    "            out = (out - min_max[0]) / (min_max[1] - min_max[0])\n",
    "            \n",
    "            psnr = calculate_psnr(out, Y, crop_border=0, input_order='HWC')\n",
    "            psnr_v2 = PSNR(np.array(out.detach().cpu()), np.array(Y.detach().cpu()), border=0)\n",
    "            ssim_v2 = SSIM(np.array(out.detach().cpu()), np.array(Y.detach().cpu()), border=0)\n",
    "            \n",
    "            if curr_epoch in [0,1]:\n",
    "                psnr_v2_origin = PSNR(np.array(_X_origin.detach().cpu()), np.array(Y.detach().cpu()), border=0)\n",
    "                ssim_v2_origin = SSIM(np.array(_X_origin.detach().cpu()), np.array(Y.detach().cpu()), border=0)\n",
    "                list_psnr_origin.append(psnr_v2_origin)\n",
    "                list_ssim_origin.append(ssim_v2_origin)\n",
    "            list_psnr.append(psnr)\n",
    "            list_psnr_v2.append(psnr_v2)\n",
    "            list_ssim_v2.append(ssim_v2)\n",
    "\n",
    "            final_ssim=0\n",
    "            \n",
    "    final_psnr_temp = sum(list_psnr)/len(list_psnr)\n",
    "    final_psnr_v2 = sum(list_psnr_v2)/len(list_psnr_v2)\n",
    "    final_ssim_v2 = sum(list_ssim_v2)/len(list_ssim_v2)\n",
    "    if curr_epoch ==0 :\n",
    "        final_psnr_v2_origin = sum(list_psnr_origin)/len(list_psnr_origin)\n",
    "        final_ssim_v2_origin = sum(list_ssim_origin)/len(list_ssim_origin)\n",
    "        log.write(f'===> final_psnr_v2_oigin: {final_psnr_v2_origin}, final_ssim_v2_origin: {final_ssim_v2_origin} \\n')\n",
    "            \n",
    "    log.write(f'===> final_psnr_temp(origin matric) {final_psnr_temp} \\n')\n",
    "    log.write(f'===> psnr_v2: {final_psnr_v2}, ssim_v2: {final_ssim_v2} \\n')\n",
    "    return final_psnr_v2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"weight file would be saved at {snap_path}\")\n",
    "\n",
    "# training\n",
    "best_psnr = 0\n",
    "start_epoch = start_epoch+1\n",
    "args.epochs = 840\n",
    "print(f'start epoch is {start_epoch} | final epoch is {args.epochs}')\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    \n",
    "    model_Enc.train()\n",
    "    model_Dec_SR.train()\n",
    "    psnr_all=[]\n",
    "    # generator\n",
    "    model_Enc.train()\n",
    "    model_Dec_Id.train()\n",
    "    model_Dec_SR.train()\n",
    "\n",
    "    # discriminator\n",
    "    model_Disc_feat.train()\n",
    "    model_Disc_img_LR.train()\n",
    "    model_Disc_img_HR.train()\n",
    "    \n",
    "    running_loss_D_total = 0.0\n",
    "    running_loss_G_total = 0.0\n",
    "\n",
    "    running_loss_align = 0.0\n",
    "    running_loss_rec = 0.0\n",
    "    running_loss_res = 0.0\n",
    "    running_loss_sty = 0.0\n",
    "    running_loss_idt = 0.0\n",
    "    running_loss_cyc = 0.0\n",
    "\n",
    "    iter = 0\n",
    "    for data,path in (train_loader):\n",
    "        iter += 1\n",
    "        X_t, Y_s = data['img_LQ'], data['img_GT']\n",
    "        us4 = nn.Upsample(scale_factor=args.scale, mode='bicubic')\n",
    "        ds4 = nn.Upsample(scale_factor=1/args.scale, mode='bicubic')\n",
    "        X_s = ds4(Y_s)\n",
    "\n",
    "        X_t = X_t.cuda(non_blocking=True)\n",
    "        X_s = X_s.cuda(non_blocking=True)\n",
    "        Y_s = Y_s.cuda(non_blocking=True)\n",
    "        X_s_store=None\n",
    "        \n",
    "        with autocast(enabled=True):\n",
    "        # real label and fake label\n",
    "            batch_size = X_t.size(0)\n",
    "            real_label = torch.full((batch_size, 1), 1, dtype=X_t.dtype).cuda(non_blocking=True)\n",
    "            fake_label = torch.full((batch_size, 1), 0, dtype=X_t.dtype).cuda(non_blocking=True)\n",
    "            model_Disc_feat.zero_grad()\n",
    "            model_Disc_img_LR.zero_grad()\n",
    "            model_Disc_img_HR.zero_grad()\n",
    "            for i in range(args.n_disc):\n",
    "            # generator output (feature domain)\n",
    "                a=[]\n",
    "\n",
    "                F_t = model_Enc(X_t)\n",
    "                F_s = model_Enc(X_s)\n",
    "                list_output_mixed_X_s=[]\n",
    "                for idx, (_xt, _xs) in enumerate(zip(F_t, F_s)):\n",
    "                    _path, val = _Storage_disc.get_minimum_scalar(_xs.detach())\n",
    "                    if _path:\n",
    "                        if i==0: # 저장해놓고 두번째 iter돌때 쓰기위해서\n",
    "                            _, X_s_store = _Storage_disc.get_img(_path)\n",
    "                        mixed_Xt = _Storage_disc.get_mixed_img(_xt,copy.deepcopy(X_s_store),alpha=0.1)\n",
    "                    else:\n",
    "                        mixed_Xt = _xt\n",
    "                        X_s_store = copy.deepcopy(mixed_Xt.detach())\n",
    "                    dist = 0.5-val if val else 0.5\n",
    "                    list_output_mixed_X_s.append(np.array(mixed_Xt.detach().cpu()))\n",
    "                F_t = torch.tensor(np.array(list_output_mixed_X_s,dtype=np.float32), requires_grad=True, dtype=torch.float32).cuda(non_blocking=True)\n",
    "\n",
    "                output_Disc_F_t = model_Disc_feat(F_t.detach())\n",
    "                output_Disc_F_s = model_Disc_feat(F_s.detach())\n",
    "                loss_Disc_F_t = loss_MSE(output_Disc_F_t, fake_label)\n",
    "                loss_Disc_F_s = loss_MSE(output_Disc_F_s, real_label)\n",
    "                loss_Disc_feat_align = (loss_Disc_F_t + loss_Disc_F_s) / 2\n",
    "\n",
    "                Y_s_s = model_Dec_SR(F_s)\n",
    "\n",
    "                output_Disc_Y_s_s = model_Disc_img_HR(Y_s_s.detach())\n",
    "                output_Disc_Y_s = model_Disc_img_HR(Y_s)\n",
    "                loss_Disc_Y_s_s = loss_MSE(output_Disc_Y_s_s, fake_label)\n",
    "                loss_Disc_Y_s = loss_MSE(output_Disc_Y_s, real_label)\n",
    "                loss_Disc_img_rec = (loss_Disc_Y_s_s + loss_Disc_Y_s) / 2\n",
    "\n",
    "                X_s_t = model_Dec_Id(F_s)\n",
    "                output_Disc_X_s_t = model_Disc_img_LR(X_s_t.detach())\n",
    "                output_Disc_X_t = model_Disc_img_LR(X_t)\n",
    "                loss_Disc_X_s_t = loss_MSE(output_Disc_X_s_t, fake_label)\n",
    "                loss_Disc_X_t = loss_MSE(output_Disc_X_t, real_label)\n",
    "                loss_Disc_img_sty = (loss_Disc_X_s_t + loss_Disc_X_t) / 2\n",
    "\n",
    "                if args.cycle_mode:\n",
    "                    Y_s_t_s = model_Dec_SR(model_Enc(model_Dec_Id(F_s)))\n",
    "                    output_Disc_Y_s_t_s = model_Disc_img_HR(Y_s_t_s.detach())\n",
    "                    output_Disc_Y_s = model_Disc_img_HR(Y_s)\n",
    "                    loss_Disc_Y_s_t_s = loss_MSE(output_Disc_Y_s_t_s, fake_label)\n",
    "                    loss_Disc_Y_s = loss_MSE(output_Disc_Y_s, real_label)\n",
    "                    loss_Disc_img_cyc = (loss_Disc_Y_s_t_s + loss_Disc_Y_s) / 2\n",
    "\n",
    "                loss_D_total = loss_Disc_feat_align + loss_Disc_img_rec + loss_Disc_img_sty + loss_Disc_img_cyc\n",
    "                if args.cycle_mode: loss_D_total += loss_Disc_img_cyc\n",
    "\n",
    "                scaler.scale(loss_D_total).backward()\n",
    "                scaler.step(optimizer_D)\n",
    "                scaler.update()\n",
    "            scheduler_D.step()\n",
    "\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            model_Enc.zero_grad()\n",
    "            model_Dec_Id.zero_grad()\n",
    "            model_Dec_SR.zero_grad()\n",
    "            for i in range(args.n_gen):\n",
    "                list_output_mixed_X_s=[]\n",
    "                list_dist_softmax=[]\n",
    "                F_t = model_Enc(X_t)\n",
    "                F_s = model_Enc(X_s)\n",
    "                for idx, (_xt, _xs) in enumerate(zip(F_t, F_s)):\n",
    "                    _path, val = _Storage_disc.get_minimum_scalar(_xs.detach())\n",
    "                    if _path and i==0:\n",
    "                        mixed_Xt = _Storage_disc.get_mixed_img(_xt,copy.deepcopy(X_s_store),alpha=0.1)\n",
    "                    else:\n",
    "                        mixed_Xt = _xt\n",
    "                    dist = 0.5-val if val else 0.5\n",
    "                    list_dist_softmax.append(dist)\n",
    "                    list_output_mixed_X_s.append(np.array(mixed_Xt.detach().cpu()))   \n",
    "                    _Storage_disc.update_storage_by_representation_redesign(path['img_GT'][idx], feature=_xt.detach(),mode='maximum')\n",
    "                list_dist_softmax = torch.reshape(torch.tensor(list_dist_softmax),[16,1]).cuda(non_blocking=True)\n",
    "                F_t = torch.tensor(np.array(list_output_mixed_X_s,dtype=np.float32), requires_grad=True, dtype=torch.float32).cuda(non_blocking=True)\n",
    "\n",
    "                output_Disc_F_t = model_Disc_feat(F_t)\n",
    "                output_Disc_F_s = model_Disc_feat(F_s)\n",
    "                loss_G_F_t = loss_MSE(output_Disc_F_t, list_dist_softmax) #mhkim\n",
    "                loss_G_F_s = loss_MSE(output_Disc_F_s, abs(1-list_dist_softmax))\n",
    "                L_align_E = loss_G_F_t + loss_G_F_s;\n",
    "                Y_s_s = model_Dec_SR(F_s)\n",
    "\n",
    "                output_Disc_Y_s_s = model_Disc_img_HR(Y_s_s)\n",
    "                loss_L1_rec = loss_L1(Y_s.detach(), Y_s_s)\n",
    "                loss_percept_rec = loss_percept(Y_s.detach(), Y_s_s)\n",
    "                loss_G_Y_s_s = loss_MSE(output_Disc_Y_s_s, real_label)\n",
    "                L_rec_G_SR = loss_L1_rec + args.lambda_percept*loss_percept_rec + args.lambda_adv*loss_G_Y_s_s\n",
    "\n",
    "                X_t_t = model_Dec_Id(F_t)\n",
    "                L_res_G_t = loss_L1(X_t, X_t_t)\n",
    "\n",
    "                X_s_t = model_Dec_Id(F_s)\n",
    "                output_Disc_X_s_t = model_Disc_img_LR(X_s_t)\n",
    "                loss_G_X_s_t = loss_MSE(output_Disc_X_s_t, real_label)\n",
    "                L_sty_G_t = loss_G_X_s_t\n",
    "\n",
    "                F_s_tilda = model_Enc(model_Dec_Id(F_s))\n",
    "                L_idt_G_t = loss_L1(F_s, F_s_tilda)\n",
    "\n",
    "                if args.cycle_mode:\n",
    "                    Y_s_t_s = model_Dec_SR(model_Enc(model_Dec_Id(F_s)))\n",
    "                    output_Disc_Y_s_t_s = model_Disc_img_HR(Y_s_t_s)\n",
    "                    loss_L1_cyc = loss_L1(Y_s.detach(), Y_s_t_s)\n",
    "                    loss_percept_cyc = loss_percept(Y_s.detach(), Y_s_t_s)\n",
    "                    loss_Y_s_t_s = loss_MSE(output_Disc_Y_s_t_s, real_label)\n",
    "                    L_cyc_G_t_G_SR = loss_L1_cyc + args.lambda_percept*loss_percept_cyc + args.lambda_adv*loss_Y_s_t_s\n",
    "            loss_G_total = args.lambda_align*L_align_E + args.lambda_rec*L_rec_G_SR + args.lambda_res*L_res_G_t + args.lambda_sty*L_sty_G_t + args.lambda_idt*L_idt_G_t\n",
    "            if args.cycle_mode: loss_G_total += args.lambda_cyc*L_cyc_G_t_G_SR\n",
    "            scaler.scale(loss_G_total).backward()\n",
    "            scaler.step(optimizer_G)\n",
    "            scaler.update()\n",
    "        scheduler_G.step()\n",
    "    \n",
    "        running_loss_D_total += loss_D_total.item()\n",
    "        running_loss_G_total += loss_G_total.item()\n",
    "\n",
    "        running_loss_align += L_align_E.item()\n",
    "        running_loss_rec += L_rec_G_SR.item()\n",
    "        running_loss_res += L_res_G_t.item()\n",
    "        running_loss_sty += L_sty_G_t.item()\n",
    "        running_loss_idt += L_idt_G_t.item()\n",
    "        running_loss_cyc += L_cyc_G_t_G_SR.item() if args.cycle_mode else 0\n",
    "        psnr = calculate_psnr(Y_s * 255, Y_s_s * 255, crop_border=0, input_order='HWC')\n",
    "        psnr_all.append(psnr)\n",
    "        \n",
    "    if (epoch+1) % args.save_freq == 0:\n",
    "        weights_file_name = 'epoch_%d.pth' % (epoch+1)\n",
    "        weights_file = os.path.join(snap_path,  weights_file_name)\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "\n",
    "            'model_Enc': model_Enc.state_dict(),\n",
    "            'model_Dec_Id': model_Dec_Id.state_dict(),\n",
    "            'model_Dec_SR': model_Dec_SR.state_dict(),\n",
    "            'model_Disc_feat': model_Disc_feat.state_dict(),\n",
    "            'model_Disc_img_LR': model_Disc_img_LR.state_dict(),\n",
    "            'model_Disc_img_HR': model_Disc_img_HR.state_dict(),\n",
    "\n",
    "            'optimizer_D': optimizer_D.state_dict(),\n",
    "            'optimizer_G': optimizer_G.state_dict(),\n",
    "\n",
    "            'scheduler_D': scheduler_D.state_dict(),\n",
    "            'scheduler_G': scheduler_G.state_dict(),\n",
    "            \n",
    "            'storage_disc': _Storage_disc,\n",
    "            'storage_gene': _Storage_gene,\n",
    "        }, weights_file)\n",
    "        log.write('save weights of epoch %d' % (epoch+1))\n",
    "    \n",
    "    ### validation\n",
    "    if epoch > 460:\n",
    "        mean_psnr = Test_v2(test_loader, snap_path.replace('weights','results'), log, \\\n",
    "                            [model_Enc, model_Dec_SR], curr_epoch = epoch+1, show_mode=False, n_interval=10, save_mode = True)\n",
    "\n",
    "        if mean_psnr>best_psnr:\n",
    "            weights_file_name = 'epoch_%d_BEST_PSNR.pth' % (epoch+1)\n",
    "            weights_file = os.path.join(snap_path,  weights_file_name)\n",
    "            best_psnr = mean_psnr\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "\n",
    "                'model_Enc': model_Enc.state_dict(),\n",
    "                'model_Dec_Id': model_Dec_Id.state_dict(),\n",
    "                'model_Dec_SR': model_Dec_SR.state_dict(),\n",
    "                'model_Disc_feat': model_Disc_feat.state_dict(),\n",
    "                'model_Disc_img_LR': model_Disc_img_LR.state_dict(),\n",
    "                'model_Disc_img_HR': model_Disc_img_HR.state_dict(),\n",
    "\n",
    "                'optimizer_D': optimizer_D.state_dict(),\n",
    "                'optimizer_G': optimizer_G.state_dict(),\n",
    "\n",
    "                'scheduler_D': scheduler_D.state_dict(),\n",
    "                'scheduler_G': scheduler_G.state_dict(),\n",
    "\n",
    "                'storage_disc': _Storage_disc,\n",
    "                'storage_gene': _Storage_gene,\n",
    "            }, weights_file)\n",
    "            log.write('save weights of epoch %d \\n ' % (epoch+1))\n",
    "        log.write('===> TRAIN epoch:%d, psnr: %f, lr:%f, loss_D_total:%f, loss_G_total:%f, loss_align:%f, loss_rec:%f, loss_res:%f, loss_sty:%f, loss_idt:%f, loss_cyc:%f \\n ' %\\\n",
    "            (epoch, sum(psnr_all)/len(psnr_all), optimizer_G.param_groups[0]['lr'], running_loss_D_total/iter, running_loss_G_total/iter, running_loss_align/iter, running_loss_rec/iter, running_loss_res/iter, running_loss_sty/iter, running_loss_idt/iter, running_loss_cyc/iter))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
